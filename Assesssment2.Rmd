---
title: "assessment2"
author: "Yewei, Linlan, Tianhan"
date: "19/12/2019"
output: html_document
---


1. preprocessing

```{r}
library(caret) # kdd0 preprocessing and Analysis models
library(dplyr) #filter
library(party)
library(randomForest) #randomForest
```

1.1 read data and check
```{r}
kdd<-read.csv("kddcup.data_10_percent.gz")
kddnames=read.table("kddcup.names",sep=":",skip=1,as.is=T)
colnames(kdd)=c(kddnames[,1],"label")
sum(is.na(kdd)) #check
```

1.2 label
```{r}
kdd0 <- kdd %>% 
mutate(label = as.character(label)) 
for (x in c("back.","land.","neptune.","pod.","smurf.","teardrop.")) {
  kdd0$label[kdd0$label == x]<-"dos."
}
for (x in c("buffer_overflow.","perl.","loadmodule.","rootkit.")) {
  kdd0$label[kdd0$label == x]<-"u2r."
}
for (x in c("ftp_write.","guess_passwd.","imap.","multihop.","phf.","spy.","warezclient.","warezmaster.")) {
  kdd0$label[kdd0$label == x]<-"r2l."
}
for (x in c("ipsweep.","nmap.","portsweep.","satan.")) {
  kdd0$label[kdd0$label == x]<-"probe."
  }
kdd0 <- kdd0 %>% 
mutate(label = as.factor(label)) 

table(kdd0$label)
```

1.3 factors

dummy variables will sparse information so directly use numeric
```{r}
kdd_<- kdd0
kdd_[,2] <- as.numeric(kdd_[,2])
kdd_[,3] <- as.numeric(kdd_[,3])
kdd_[,4] <- as.numeric(kdd_[,4])
```


1.4 use randomforest importance get the Principal Component

1.4.1 firstly give function of Split dataset for training and testing

```{r}
Traindata<-function(data,p=0.10,seed=1){
set.seed(seed) 
train_rows <- sample(1:nrow(data), size = p*nrow(data))
return(train_rows)
}
```

```{r}
Traindata_kdd <- function(data,p=0.10,seed=1){
Train_data <- NULL
Test_data <- NULL
for (a in names(table(kdd_$label))) {
  data_ <- filter(data,label==a)
  Traindata <- data_[Traindata(data_,p,seed),]
  Testdata <- data_[-Traindata(data_,p,seed),]
  Train_data <- rbind(Train_data,Traindata)
  Test_data <- rbind(Test_data,Testdata)
}  
Train_data <<- Train_data 
Testdata_kdd <<- Test_data 
return(Train_data)
}
```

1.4.2 creat a randomforest
```{r}
# Create the forest.
  rfFit <- randomForest(label ~ duration + protocol_type + service	+ flag + src_bytes + dst_bytes + land + wrong_fragment + logged_in + urgent + hot + num_failed_logins + logged_in + num_compromised + root_shell + su_attempted + num_root + num_file_creations + num_shells + num_access_files + num_outbound_cmds + is_host_login + is_guest_login + count + srv_count + serror_rate + srv_serror_rate + rerror_rate + srv_rerror_rate + same_srv_rate + srv_diff_host_rate + diff_srv_rate + dst_host_count + dst_host_srv_count + dst_host_same_srv_rate + dst_host_diff_srv_rate + dst_host_same_src_port_rate + dst_host_srv_diff_host_rate + dst_host_serror_rate + dst_host_srv_serror_rate + dst_host_rerror_rate + dst_host_srv_rerror_rate,
                      data = Traindata_kdd(kdd_,p=0.1,seed=1))
# View the forest results.
print(rfFit) 
```

1.4.3 importance analysis

```{R}
inames=NULL
ipr=NULL
for (i in 1:5) {
  rfFit <- randomForest(label ~ duration + protocol_type + service	+ flag + src_bytes + dst_bytes + land + wrong_fragment + logged_in + urgent + hot + num_failed_logins + logged_in + num_compromised + root_shell + su_attempted + num_root + num_file_creations + num_shells + num_access_files + num_outbound_cmds + is_host_login + is_guest_login + count + srv_count + serror_rate + srv_serror_rate + rerror_rate + srv_rerror_rate + same_srv_rate + srv_diff_host_rate + diff_srv_rate + dst_host_count + dst_host_srv_count + dst_host_same_srv_rate + dst_host_diff_srv_rate + dst_host_same_src_port_rate + dst_host_srv_diff_host_rate + dst_host_serror_rate + dst_host_srv_serror_rate + dst_host_rerror_rate + dst_host_srv_rerror_rate,
                      data = Traindata_kdd(kdd_,p=0.08,seed=i))
  
  importance <- as.data.frame(t(importance(rfFit,type = 2)))
  importance <- sort(importance, decreasing = T)
  inames <- rbind(inames,names(importance))
  pr=NULL
  for (j in 1:41) {
    pr[j]=sum(importance[1:j])/sum(importance)
  }
  ipr <- rbind(ipr,pr)
}

```


```{R}
inames <- as.data.frame(inames)
ipr <- as.data.frame(ipr)
```

Find importance proportion： 90%:15 95%:19 99%:26 1:41

```{R}
data <- as.matrix(inames[1:19])
```


```{R}
dim(data) <- c(5*19,1)

data <- as.data.frame(data)
table(data)
```
The first 18 don not argue, choose the 19th one in order

1.5 Complete preprocessing, give trainingset and testset

```{R}
kdd_pro <- kdd_[,names(kdd_) %in% t(as.matrix(cbind(inames[1:18],"dst_host_serror_rate")))[1:19]]
label <- kdd_$label
hot_code <- model.matrix(~label-1,kdd_pro) %>% as.data.frame()
kdd_pro <- cbind(kdd_pro,label)
```


```{R}
Train_data <- Traindata_kdd(kdd_pro,p=0.7)
Test_data <- Testdata_kdd

```


2.Making model

```{r}
library(rpart)
library(rpart.plot)
library(survival)
library(gbm)
library(xgboost)
library(adabag)
library(ggplot2)
```

2.1 baseline model (Decision tree)

```{r}
do.stuff0 <- function(){
fit  <<-  rpart(label~.,data=Train_data, method = 'class')}
system.time(do.stuff0())
```

```{r}
#Model Summary
summary(fit)
rpart.plot(fit)
fit$variable.importance
```
```{r}
kdd_srv <- kdd_pro[,c(7,20)]
ggplot(kdd_srv, aes(x = count, y = label, color=label))+geom_point()
```

2.2 Gradient Boosting Descion Tree

```{r}
do.stuff1 <- function(){
label.gbm  <<-  gbm(formula = Train_data$label~ .,distribution = "multinomial",data = Train_data ,n.trees = 100,interaction.depth = 3,shrinkage = 0.01,cv.folds = 2)
}
system.time(do.stuff1())
```

```{r}
summary(label.gbm)
label.iter = gbm.perf(label.gbm,method = "cv")
print(label.iter)  
```


2.3 XGBoost

```{r}
do.stuff2 <- function(){
xgb <<-xgboost(data = as.matrix(Train_data[,1:19]), 
 label = as.numeric(Train_data[,20]), 
 eta = 0.1,
 max_depth = 15, 
 nround=25, 
 subsample = 0.7,  
 colsample_bytree = 0.5,
 seed = 1,
 eval_metric = "merror",
 objective = "multi:softmax",
 num_class = 12,
 nthread = 3
)}
system.time(do.stuff2())
```

```{r}
names <- names(kdd_pro[,-1])
importance_matrix <- xgb.importance(names, model = xgb)
xgb.plot.importance(importance_matrix[1:19,])
```


find it is different from above two models,
Drawing scatter diagram to find the characteristics of principal components
```{r}
kdd_srv <- kdd_pro[,c(8,20)]
ggplot(kdd_srv, aes(x = srv_count, y = label, color=label))+geom_point()

kdd_srv_log <- kdd_pro[,c(8,6,20)]
ggplot(kdd_srv_log, aes(x = srv_count, y = logged_in, color=label))+geom_point()

kdd_srv_rate <- kdd_pro[,c(8,9,20)]
ggplot(kdd_srv_rate, aes(x = srv_count, y = serror_rate, color=label))+geom_point()

kdd_srv_rate <- kdd_pro[,c(8,7,20)]
ggplot(kdd_srv_rate, aes(x = srv_count, y = count, color=label))+geom_point()
```

```{r}
ggplot(kdd_srv, aes(x = srv_count,fill = label)) +
  geom_histogram(position = "identity", alpha = 0.4)
```

```{r}
ggplot(kdd_srv, aes(x = srv_count)) +
  geom_histogram(fill = "lightblue", colour = "black") +
  facet_grid(label ~ .)
```

```{r}
ggplot(kdd_srv, aes(x = srv_count, fill = label)) +
  geom_density(alpha = 0.3)
```

```{r}
ggplot(kdd_srv, aes(x = label, y = srv_count, fill = label)) +
  geom_boxplot(notch = TRUE) +
  scale_fill_brewer(palette = "Pastel2")
```


2.4 Adaboost:
```{r}
do.stuff2 <- function(){
ada <<- boosting(label~., data=Train_data, boos=F, mfinal=5)#have tried boos with F and T，F is better
}
system.time(do.stuff2())
```
```{r}
summary(ada)
ada$importance
barplot(ada$importance)   
```

the importance is similar to former 2 model.

3. Metric (confusionMatrix)

3.1 baseline
```{r}
baseline = predict(fit, Test_data[,1:19], type = 'class')
print(confusionMatrix(as.factor(as.numeric(baseline)),as.factor(as.numeric(Test_data[,20]))))
```

3.2 gbm
```{r}
label.predict = predict(label.gbm,Testdata_kdd,n.trees = label.iter)
#str(label.predict)
solution <- as.factor(colnames(label.predict)[apply(label.predict,1,which.max)])
#print(solution)

print(confusionMatrix(solution,as.factor(Testdata_kdd$label)))
```

3.3 xgboost

```{r}
xg_pred <- predict(xgb,as.matrix(Test_data[,1:19]))
print(confusionMatrix(as.factor(xg_pred),as.factor(as.numeric(Test_data[,20]))))
```

3.4 adaboost
```{r}
ada_pred <- predict(ada,Test_data[,1:19])$class
print(confusionMatrix(as.factor(as.numeric(as.factor(ada_pred))),as.factor(as.numeric(Test_data[,20]))))
```










